{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import torch\n",
    "from protera_stability.config.lazy import LazyCall as L\n",
    "from protera_stability.config.common.mlp import mlp_esm\n",
    "from protera_stability.train import get_cfg, setup_diversity, setup_data\n",
    "\n",
    "exp_params = {\n",
    "    \"diversity_cutoff\": 0.866,\n",
    "    \"random_percent\": 0.15,\n",
    "    \"sampling_method\": \"diversity\",\n",
    "    \"experiment_name\": \"base\",\n",
    "}\n",
    "\n",
    "def create_cfg(exp_params):\n",
    "    cfg = get_cfg(args={})\n",
    "    cfg = setup_diversity(cfg, **exp_params)\n",
    "    mlp_esm.n_units = 2048\n",
    "    mlp_esm.act = L(torch.nn.GELU)()\n",
    "    cfg.model = mlp_esm\n",
    "\n",
    "    cfg = setup_data(cfg)\n",
    "    return cfg\n",
    "\n",
    "cfg = create_cfg(exp_params)\n",
    "cfg.keys()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['trainer_params', 'output_dir', 'random_split', 'experiment', 'model', 'dataloader'])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from protera_stability.train import do_train\n",
    "trainer = do_train(cfg)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/roberto/anaconda3/envs/protera-stability/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory logs/base_all-data/models exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/roberto/anaconda3/envs/protera-stability/lib/python3.8/site-packages/pytorch_lightning/utilities/seed.py:57: UserWarning: No correct seed found, seed set to 634843590\n",
      "  rank_zero_warn(f\"No correct seed found, seed set to {seed}\")\n",
      "Global seed set to 634843590\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== USING diversity as Sampling Method ===\n",
      "=== USING 6137 out of 8204 samples ===\n",
      "=== SIZE WAS DETERMINED BY CUTOFF ===\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "  | Name     | Type       | Params\n",
      "----------------------------------------\n",
      "0 | model    | ProteinMLP | 4.7 M \n",
      "1 | train_r2 | R2Score    | 0     \n",
      "2 | valid_r2 | R2Score    | 0     \n",
      "3 | test_r2  | R2Score    | 0     \n",
      "----------------------------------------\n",
      "4.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.7 M     Total params\n",
      "18.891    Total estimated model params size (MB)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                                      "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Global seed set to 634843590\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Global seed set to 634843590\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 14: 100%|██████████| 28/28 [00:02<00:00, 14.17it/s, loss=0.246, v_num=2, train/r2=0.732, train/loss=0.254, valid/r2=0.695, valid/loss=0.279]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "trainer.test()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/roberto/anaconda3/envs/protera-stability/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.prepare_data has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.prepare_data.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing:  20%|██        | 1/5 [00:00<00:01,  2.16it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test/loss': 0.3312093913555145,\n",
      " 'test/r2': 0.6877349615097046,\n",
      " 'test/r2_step': 0.6877349615097046}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 5/5 [00:00<00:00,  8.12it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<protera_stability.engine.default.DefaultTrainer at 0x7fb435418fa0>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "exp_params[\"sampling_method\"] = \"random\"\n",
    "cfg = create_cfg(exp_params)\n",
    "\n",
    "trainer = do_train(cfg)\n",
    "trainer.test()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/roberto/anaconda3/envs/protera-stability/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory logs/base_random_0.15/models exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name     | Type       | Params\n",
      "----------------------------------------\n",
      "0 | model    | ProteinMLP | 4.7 M \n",
      "1 | train_r2 | R2Score    | 0     \n",
      "2 | valid_r2 | R2Score    | 0     \n",
      "3 | test_r2  | R2Score    | 0     \n",
      "----------------------------------------\n",
      "4.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.7 M     Total params\n",
      "18.891    Total estimated model params size (MB)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== USING random as Sampling Method ===\n",
      "=== USING 2461 out of 8204 samples ===\n",
      "=== SIZE WAS DETERMINED BY RANDOM PERCENT OF 0.15 ===\n",
      "                                                                      "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Global seed set to 634843590\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 8: 100%|██████████| 14/14 [00:02<00:00,  7.28it/s, loss=0.334, v_num=1, train/r2=0.674, train/loss=0.333, valid/r2=0.756, valid/loss=0.235]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/roberto/anaconda3/envs/protera-stability/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.prepare_data has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.prepare_data.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing:  20%|██        | 1/5 [00:00<00:02,  1.72it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test/loss': 0.23753789067268372,\n",
      " 'test/r2': 0.7481482028961182,\n",
      " 'test/r2_step': 0.7481482028961182}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 5/5 [00:00<00:00,  6.32it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<protera_stability.engine.default.DefaultTrainer at 0x7fb423bff520>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "def run_experiment(data_path, epochs, max_randoms, cutoffs):\n",
    "    exp_params = {\n",
    "        \"diversity_cutoff\": 0.866,\n",
    "        \"random_percent\": 0.15,\n",
    "        \"sampling_method\": \"diversity\",\n",
    "        \"experiment_name\": \"base\",\n",
    "    }\n",
    "\n",
    "    print(\"=== RUNNING RANDOM SAMPLING ===\")\n",
    "    for random_percent in tqdm(max_randoms):\n",
    "        exp_params[\"sampling_method\"] = \"random\"\n",
    "        exp_params[\"random_percent\"] = random_percent\n",
    "\n",
    "        cfg = create_cfg(exp_params)\n",
    "\n",
    "        trainer = do_train(cfg)\n",
    "        test_results = trainer.test()\n",
    "        \n",
    "        pickle.dump(test_results, open(Path(\"../logs\") / f\"stability_random_{random_percent}\" / \"test.pkl\", \"wb\"))\n",
    "        \n",
    "    print(\"=== RUNNING DIVERSITY SAMPLING ===\")\n",
    "    for cut in tqdm(cutoffs):\n",
    "        exp_params[\"sampling_method\"] = \"diversity\"\n",
    "        exp_params[\"diversity_cutoff\"] = cut\n",
    "\n",
    "        cfg = create_cfg(exp_params)\n",
    "\n",
    "        trainer = do_train(cfg)\n",
    "        test_results = trainer.test()\n",
    "        \n",
    "        pickle.dump(test_results, open(Path(\"../logs\") / f\"stability_cut_{cut}\" / \"test.pkl\", \"wb\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "random_percents = [0.8, 0.5, 0.25, 0.15, 0.1]\n",
    "cutoffs = [0.82, 0.83, 0.85, 0.86, 0.87, 0.875, 0.878]\n",
    "\n",
    "run_experiment(\n",
    "    data_path=data_path, epochs=150, max_randoms=random_percents, cutoffs=cutoffs\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('protera-stability': conda)"
  },
  "interpreter": {
   "hash": "a78d51e19256cfce1d39a2c3cba2e8056cac1355cb83723ad6d6938cbf3c5352"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}